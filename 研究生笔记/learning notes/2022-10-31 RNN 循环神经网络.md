---
layout:     post
title:      2022-10-31 循环神经网络
subtitle:   循环神经网络是一种经典的神经网络结构
description: 
date:       2022-10-31
categories: deeplearning
tags: RNN，LSTM
comments: true
---

# 2022-10-31 循环神经网络

> 循环神经网络是一种经典的神经网络、用来处理序列输入问题

## 什么是循环神经网络
循环神经网络（Rerrent Neural Network, RNN），历史啊，谁发明的都不重要，说了你也记不住，你只要记住RNN是神经网络的一种，类似的还有深度神经网络DNN，卷积神经网络CNN，生成对抗网络GAN，等等。另外你需要记住RNN的特点，**RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，**利用了RNN的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等NLP领域的问题时有所突破。

我们需要重点来了解一下RNN的特点这句话，什么是**序列特性**呢？我个人理解，就是**符合时间顺序，逻辑顺序，或者其他顺序就叫序列特性**，举几个例子：

-   拿人类的某句话来说，也就是人类的自然语言，是不是符合某个逻辑或规则的字词拼凑排列起来的，这就是符合序列特性。
-   语音，我们发出的声音，每一帧每一帧的衔接起来，才凑成了我们听到的话，这也具有序列特性、
-   股票，随着时间的推移，会产生具有顺序的一系列数字，这些数字也是具有序列特性。
* A preview of what's to come
## 为什么需要循环神经网络

循环神经网络可以用来处理时序信息，即输入长度不固定的信息。

为什么呢？
比如一个简单的MLP,在网络确定好之后、他的输入输出大小就固定了。当碰到了语音这种时序信息、数据就会有长有短、原有的网络结构就不太适用于这种数据的处理、而循环神经网络凭借他的参数共享设计、就可以处理任意长度的数据。

## 循环神经网络的结构
### 经典结构如下

![[Pasted image 20221031143033.png]]
看下面这个展开的图更好理解

![[Pasted image 20221031143225.png]]
*必须要理解的一件事就是，这里的RNN网络结构都是同一个，他们是参数共享的*
### 计算公式
$h_t=f_w(h_{t-1},x_t)$

$h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)$

$y_t=W_{hy}h_t$


* Examples
* Transition to the next section
### 计算图
#### many to many

![[Pasted image 20221031143708.png]]
#### many to one
![[Pasted image 20221031143843.png]]
#### one to many
![[Pasted image 20221031143903.png]]
## RNN 优缺点

- 优点
	- Can process any length input 
	- Computation for step t can (in theory) use information from many steps back 
	- Model size doesn’t increase for longer input 
	- Same weights applied on every timestep, so there is symmetry in how inputs are processed.
* 缺点
	* Recurrent computation is slow
	- In practice, difficult to access information from many steps back
## LSTM

![[Pasted image 20221031150008.png]]
![[Pasted image 20221031150752.png]]
上图就是经典的LSTM结构、总共有$i,f,o,g$四个门来控制
*遗忘门* : $f$ 来决定之前的$c_{t-1}$当前需要遗忘多少
*记忆门*：$i$ 决定当前的输入信息有哪些是需要记住的
*输出门*：$o$ 决定了当前哪些信息是需要输出的

## Summary
![[Pasted image 20221031213311.png]]
- RNNs allow a lot of flexibility in architecture design

- Vanilla RNNs are simple but don’t work very well

- Common to use LSTM or GRU: their additive interactions

improve gradient flow

- Backward flow of gradients in RNN can explode or vanish.

Exploding is controlled with gradient clipping. Vanishing is

controlled with additive interactions (LSTM)

- Better/simpler architectures are a hot topic of current research,

as well as new paradigms for reasoning over sequences

- Better understanding (both theoretical and empirical) is needed.

