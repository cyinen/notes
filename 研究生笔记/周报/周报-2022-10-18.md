汇报人：陈银

日期：2022-10-18

## 信息输入

### 1、CS231N 的Course Materials阅读

上周基本完成了CS231N的整体课程的学习，目前主要是在阅读课程的其他补充材料、主要是对自己感兴趣的部分仔细阅读。最近在看他的模型训练的技巧、以及梯度下降的Trick这几篇材料。如下

Neural Networks, Parts [==1==](http://cs231n.github.io/neural-networks-1), [==2==](http://cs231n.github.io/neural-networks-2), [==3==](http://cs231n.github.io/neural-networks-3)

Suggested Readings:

1. [==Stochastic Gradient Descent Tricks==](http://research.microsoft.com/pubs/192769/tricks-2012.pdf)
2. [==Efficient Backprop==](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
3. [==Practical Recommendations for Gradient-based Training==](https://arxiv.org/pdf/1206.5533v2.pdf)
4. [==Deep Learning, Nature 2015==](https://www.nature.com/articles/nature14539)
5. [==An Overview of Gradient Descent Algorithms==](https://ruder.io/optimizing-gradient-descent/)
6. [==A Disciplined Approach to Neural Network Hyper-Parameters==](https://arxiv.org/abs/1803.09820)



> 来自 <[http://cs231n.stanford.edu/schedule.html](http://cs231n.stanford.edu/schedule.html)\>

### 2、前言论文阅读

阅读了最新的 Activating More Pixels in Image Super-Resolution Transformer

### 3、通用技术的学习

  1）学习了如何把一个pytorch模型转换为paddle格式的模型，并使用paddle框架进行模型的训练

  2）学习docker的使用、使用docker在服务器上训练


## 信息输出

主要是 模型训练的一些trick， 还有常见算子的反向传播实现笔记:

## 1.Randomly shuffle the training examples. 随机挑选数据、增强数据的diversity

## 2.Use preconditioning techniques.

使用先验知识：

## 3.监视你的训练、定期查看在验证集上的错误率 Monitor both the training cost and the validation error



![](image_1.7f3889f1.png)

## 4.Check the gradients using finite differences 有限差分检查梯度

![](image_2.a06aa0f0.png)



## 5.Experiment with the learning rates γt using a small sample of the training set.

![](image_3.19c18771.png)



![](image_4.f34c3213.png)



![](image_5.dbf3be93.png)








-----


